# This viminfo file was generated by Vim 7.4.
# You may edit it if you're careful!

# Value of 'encoding' when this file was written
*encoding=utf-8


# hlsearch on (H) or off (h):
~h
# Last Search Pattern:
~MSle0~/bin

# Command Line History (newest to oldest):
:wq
:Wq
:q!
:Q!
:q1
:WQ
:q

# Search String History (newest to oldest):
?/bin/bash
?/distinct
?/sys.argv
?/argv
?/[2]
?/JAVA

# Expression History (newest to oldest):

# Input Line History (newest to oldest):

# Input Line History (newest to oldest):

# Registers:
"0	CHAR	0
	"default_partition_pagerank.py" 
"1	CHAR	0
	 and runs a pagrank algorithm on it.
	
"2	CHAR	0
	"custom_partition_pagerank.py" is a pyspark script that uses custom partitioner in pagerank algorithm
	"default_partition_pagerank.py" is a pyspark script that uses the default  partitioner in pagerank algorithm
"3	CHAR	0
	dd	The script takes the input file, puts it in hdfs. Then a spark-submit job is run that takes the hdfs file as input performs the pagerank computation
		that stores the result in the hdfs folder. The script regains the control and gets the output from hdfs and stores it back in the local fs.
	
"4	LINE	0
	file=$1
"5	CHAR	0
	where $3 is the name of file which 
	
"6	CHAR	0
	ame of the output folder where you want to save the output(default is "output")
	
"7	CHAR	0
	 name if the input file( default is 'web-BerkStan.txt' )
	
"8	LINE	0
	hdfs dfs -get /output $OUTPUT	# get the directory from hdfs directory into a folder named $OUTPUT in local fs
"9	LINE	0
	rm -rf $OUTPUT 		# remove the output directory if it already exists
""-	CHAR	0
	 Task2_Task3_Enwiki

# File marks:
'0  34  32  ~/part3/Task2_Task3_Enwiki/default_partition_pagerank.py
'1  20  57  ~/part3/Task2_Task3_Enwiki/run.sh
'2  15  4  ~/part3/Task2_Task3_Enwiki/run.sh
'3  2  4  ~/part2/.gitignore
'4  3  0  ~/part3/README
'5  8  72  ~/part3/README
'6  1  97  ~/part3/README
'7  6  23  ~/part3/Task1-Berkstan/README
'8  1  0  ~/part3/Task1-Berkstan/pagerank.py
'9  1  0  ~/part3/Task1-Berkstan/run.sh

# Jumplist (newest first):
-'  34  32  ~/part3/Task2_Task3_Enwiki/default_partition_pagerank.py
-'  1  0  ~/part3/Task2_Task3_Enwiki/default_partition_pagerank.py
-'  20  57  ~/part3/Task2_Task3_Enwiki/run.sh
-'  1  0  ~/part3/Task2_Task3_Enwiki/run.sh
-'  15  4  ~/part3/Task2_Task3_Enwiki/run.sh
-'  2  4  ~/part2/.gitignore
-'  1  0  ~/part2/.gitignore
-'  3  0  ~/part3/README
-'  1  0  ~/part3/README
-'  8  72  ~/part3/README
-'  6  23  ~/part3/Task1-Berkstan/README
-'  1  0  ~/part3/Task1-Berkstan/README
-'  1  0  ~/part3/Task1-Berkstan/pagerank.py
-'  1  0  ~/part3/Task1-Berkstan/run.sh
-'  2  15  ~/part3/Task1-Berkstan/.gitignore
-'  1  0  ~/part3/Task1-Berkstan/.gitignore
-'  24  0  ~/part3/Task2_Task3_Enwiki/README
-'  1  0  ~/part3/Task2_Task3_Enwiki/README
-'  20  0  ~/part3/Task1-Berkstan/README
-'  14  139  ~/part3/Task1-Berkstan/README
-'  15  63  ~/part3/Task1-Berkstan/README
-'  19  66  ~/part3/Task2_Task3_Enwiki/README
-'  58  22  ~/part3/Task2_Task3_Enwiki/default_partition_pagerank.py
-'  58  22  ~/part3/Task2_Task3_Enwiki/custom_partition_pagerank.py
-'  1  0  ~/part3/Task2_Task3_Enwiki/custom_partition_pagerank.py
-'  18  13  ~/part3/Task2_Task3_Enwiki/run.sh
-'  25  65  ~/part3/Task2_Task3_Enwiki/README
-'  10  159  ~/part3/Task2_Task3_Enwiki/README
-'  14  0  ~/part3/Task2_Task3_Enwiki/README
-'  11  1  ~/part3/Task2_Task3_Enwiki/README
-'  12  0  ~/part3/Task2_Task3_Enwiki/README
-'  21  0  ~/part3/Task2_Task3_Enwiki/run.sh
-'  4  0  ~/part3/Task2_Task3_Enwiki/run.sh
-'  27  19  ~/part3/Task2_Task3_Enwiki/custom_partition_pagerank.py
-'  13  0  ~/part3/Task2_Task3_Enwiki/default_partition_pagerank.py
-'  19  65  ~/part2/README
-'  1  0  ~/part2/README
-'  13  83  ~/part2/part2.sh
-'  1  0  ~/part2/part2.sh
-'  6  13  ~/part2/CSV_sort.py
-'  1  0  ~/part2/CSV_sort.py
-'  5  102  ~/part2/CSV_sort.py
-'  6  0  ~/part2/part2.sh
-'  4  0  /etc/hosts
-'  1  0  /etc/hosts
-'  2  33  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/workers
-'  1  0  /mnt/datahadoop-3.1.2/etc/hadoop/workers
-'  1  0  hadoop-3.1.2/etc/hadoop/workers
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/core-site.xml
-'  1  0  hadoop-3.1.2/etc/hadoop/core-site.xml
-'  9  445  ~/.ssh/authorized_keys
-'  1  0  ~/.ssh/authorized_keys
-'  3  8  ~/all_ips
-'  1  0  ~/all_ips
-'  7  0  ~/part2/CSV_sort.py
-'  12  76  ~/part2/part2.sh
-'  117  1  ~/.bashrc
-'  1  0  ~/.bashrc
-'  91  0  ~/.bashrc
-'  11  140  ~/part2/README
-'  56  0  ~/pagerank_custome_p.py
-'  1  0  ~/pagerank_custome_p.py
-'  20  67  ~/part2/README
-'  4  48  ~/part2/README
-'  14  38  ~/part2/CSV_sort.py
-'  17  104  ~/part2/part2.sh
-'  71  97  ~/pagerank_custome_p.py
-'  63  104  ~/pagerank_custome_p.py
-'  31  0  ~/pagerank_custome_p.py
-'  67  86  ~/pagerank_custome_p.py
-'  69  15  ~/pagerank_custome_p.py
-'  54  107  ~/pagerank_nopartition.py
-'  1  0  ~/pagerank_nopartition.py
-'  1  10  ~/.vimrc
-'  69  0  ~/pagerank_nopartition.py
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf
-'  24  0  /mnt/data/spark_logs/app-20190922184312-0000
-'  1  0  /mnt/data/spark_logs/app-20190922184312-0000
-'  1  0  /mnt/data/hadoop-3.1.2/sbin/start-dfs.sh
-'  2  8  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
-'  4  8  /mnt/data/hadoop-3.1.2/etc/hadoop/workers
-'  22  22  /mnt/data/hadoop-3.1.2/etc/hadoop/core-site.xml
-'  34  44  ~/mod3.py
-'  1  0  ~/mod3.py
-'  69  15  ~/mod3.py
-'  75  86  ~/mod3.py
-'  69  15  ~/pagerank_large_keyBy.py
-'  1  0  ~/pagerank_large_keyBy.py
-'  5  75  ~/run_one_job.sh
-'  1  0  ~/run_one_job.sh
-'  63  56  ~/pagerank_large_keyBy.py
-'  64  4  ~/pagerank_large_keyBy.py
-'  7  0  ~/pagerank_nopartition.py
-'  6  9  ~/pagerank_nopartition.py

# History of marks within files (newest to oldest):

> ~/part3/Task2_Task3_Enwiki/default_partition_pagerank.py
	"	34	32
	^	34	33
	.	34	32
	+	58	22
	+	34	32

> ~/part3/Task2_Task3_Enwiki/run.sh
	"	20	57
	^	20	58
	.	20	56
	+	20	0
	+	6	0
	+	1	9
	+	1	23
	+	6	16
	+	7	9
	+	8	2
	+	10	13
	+	11	6
	+	7	13
	+	11	11
	+	20	85
	+	20	162
	+	22	0
	+	4	7
	+	1	39
	+	3	64
	+	5	29
	+	8	2
	+	14	10
	+	15	5
	+	15	5
	+	17	0
	+	19	25
	+	20	89
	+	21	0
	+	22	0
	+	22	0
	+	4	6
	+	1	12
	+	3	9
	+	4	0
	+	1	0
	+	1	0
	+	2	0
	+	1	104
	+	18	13
	+	20	56

> ~/part2/.gitignore
	"	2	4
	^	2	5
	.	2	4
	+	1	6
	+	2	4

> ~/part3/README
	"	3	0
	^	3	0
	.	3	0
	+	1	91
	+	1	106
	+	9	0
	+	1	55
	+	3	96
	+	4	0
	+	1	55
	+	6	27
	+	1	97
	+	8	72
	+	1	14
	+	3	0
	+	4	0
	+	3	0

> ~/part3/Task1-Berkstan/README
	"	6	23
	^	6	24
	.	4	84
	+	15	1
	+	15	2
	+	14	140
	+	15	0
	+	17	0
	+	19	0
	+	4	84

> ~/part3/Task1-Berkstan/pagerank.py
	"	1	0

> ~/part3/Task1-Berkstan/run.sh
	"	1	0

> ~/part3/Task1-Berkstan/.gitignore
	"	2	15
	^	2	16
	.	2	0
	+	2	0

> ~/part3/Task2_Task3_Enwiki/README
	"	24	0
	^	24	0
	.	23	64
	+	9	14
	+	12	17
	+	11	36
	+	16	0
	+	16	0
	+	19	66
	+	16	0
	+	9	15
	+	10	35
	+	11	7
	+	12	7
	+	10	7
	+	12	63
	+	16	0
	+	11	0
	+	10	77
	+	19	66
	+	25	0
	+	19	86
	+	14	0
	+	16	0
	+	14	93
	+	14	34
	+	16	0
	+	14	0
	+	14	43
	+	14	73
	+	15	0
	+	4	0
	+	5	108
	+	3	50
	+	3	0
	+	19	67
	+	10	159
	+	11	58
	+	12	64
	+	17	204
	+	18	92
	+	18	8
	+	19	66
	+	23	64

> ~/part3/Task2_Task3_Enwiki/custom_partition_pagerank.py
	"	58	22
	^	58	23
	.	58	22
	+	43	102
	+	45	102
	+	52	100
	+	49	71
	+	27	19
	+	58	22

> ~/part2/README
	"	19	65
	^	19	66
	.	18	132
	+	4	102
	+	15	0
	+	14	14
	+	10	0
	+	11	0
	+	15	150
	+	20	67
	+	12	46
	+	18	50
	+	16	139
	+	18	156
	+	19	65
	+	18	132

> ~/part2/part2.sh
	"	13	83
	^	12	77
	.	12	76
	+	1	11
	+	17	29
	+	13	0
	+	12	76

> ~/part2/CSV_sort.py
	"	6	13
	^	5	49
	.	5	48
	+	14	0
	+	9	28
	+	6	13
	+	5	48

> /etc/hosts
	"	4	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
	"	2	33
	^	2	34
	.	3	0
	+	1	37
	+	3	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
	"	1	0
	^	2	9
	.	2	8
	+	1	34
	+	1	9
	+	2	8

> /mnt/data/hadoop-3.1.2/etc/hadoop/workers
	"	1	0
	^	4	9
	.	4	8
	+	5	16
	+	5	0
	+	4	8

> /mnt/datahadoop-3.1.2/etc/hadoop/workers
	"	1	0

> hadoop-3.1.2/etc/hadoop/workers
	"	1	0

> /mnt/data/hadoop-3.1.2/etc/hadoop/core-site.xml
	"	1	0
	^	22	23
	.	22	22
	+	20	10
	+	23	0
	+	22	22

> hadoop-3.1.2/etc/hadoop/core-site.xml
	"	1	0

> ~/.ssh/authorized_keys
	"	9	445
	^	9	446
	.	9	429
	+	10	429
	+	9	429

> ~/all_ips
	"	3	8
	^	3	9
	.	3	9
	+	1	9
	+	3	9

> ~/.bashrc
	"	117	1
	^	118	45
	.	118	44
	+	117	2
	+	119	0
	+	117	2
	+	118	44

> ~/pagerank_custome_p.py
	"	56	0
	^	71	98
	.	71	98
	+	63	102
	+	65	103
	+	71	100
	+	69	15
	+	74	21
	+	67	87
	+	63	101
	+	65	101
	+	71	98

> ~/pagerank_nopartition.py
	"	54	107
	^	1	4
	.	7	0
	+	1	75
	+	1	87
	+	1	17
	+	1	74
	+	1	0
	+	1	57
	+	1	65
	+	1	65
	+	1	65
	+	1	41
	+	1	69
	+	1	0
	+	7	0
	+	7	27
	+	67	0
	+	30	0
	+	31	69
	+	59	0
	+	59	83
	+	61	60
	+	65	88
	+	7	0
	+	6	0
	+	1	4
	+	7	0

> ~/.vimrc
	"	1	10
	^	1	11
	.	1	11
	+	1	11

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf
	"	1	0
	^	2	19
	.	2	19
	+	6	0
	+	3	0
	+	7	30
	+	2	19

> /mnt/data/spark_logs/app-20190922184312-0000
	"	24	0

> /mnt/data/hadoop-3.1.2/sbin/start-dfs.sh
	"	1	0

> ~/mod3.py
	"	34	44
	^	34	45
	.	34	44
	+	75	0
	+	74	0
	+	72	1
	+	69	15
	+	34	44

> ~/pagerank_large_keyBy.py
	"	69	15
	^	69	16
	.	69	15
	+	74	0
	+	69	15

> ~/run_one_job.sh
	"	5	75
	^	5	76
	.	5	75
	+	1	33
	+	10	0
	+	8	0
	+	7	0
	+	6	0
	+	1	7
	+	5	0
	+	1	9
	+	2	8
	+	5	67
	+	3	8
	+	5	75

> ~/nopartition.sh
	"	1	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/sbin/spark-defaults.conf
	"	1	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf.template
	"	30	21
	^	30	22
	.	29	21
	+	24	0
	+	23	0
	+	24	99
	+	26	36
	+	27	20
	+	23	23
	+	24	19
	+	26	20
	+	29	21

> ~/part3_large.sh
	"	6	73
	^	6	74
	.	6	73
	+	6	73

> /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
	"	54	53
	^	54	54
	.	54	17
	+	54	17

> /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
	"	26	33
	^	26	34
	.	26	33
	+	27	0
	+	22	44
	+	26	33
