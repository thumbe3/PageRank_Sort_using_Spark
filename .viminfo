# This viminfo file was generated by Vim 7.4.
# You may edit it if you're careful!

# Value of 'encoding' when this file was written
*encoding=utf-8


# hlsearch on (H) or off (h):
~h
# Last Search Pattern:
~MSle0~/bin

# Command Line History (newest to oldest):
:wq
:Wq
:q!
:Q!
:q1
:WQ
:q

# Search String History (newest to oldest):
?/bin/bash
?/distinct
?/sys.argv
?/argv
?/[2]
?/JAVA

# Expression History (newest to oldest):

# Input Line History (newest to oldest):

# Input Line History (newest to oldest):

# Registers:
"0	CHAR	0
	 key_partitioner
"1	LINE	0
	c220g1-031123vm-3.wisc.cloudlab.us
"2	LINE	0
	c220g1-031123vm-2.wisc.cloudlab.us
"3	LINE	0
	c220g1-031123vm-3.wisc.cloudlab.us
"4	LINE	0
	c220g1-031123vm-2.wisc.cloudlab.us
"5	LINE	0
	c220g1-031123vm-1.wisc.cloudlab.us
"6	LINE	0
	
"7	LINE	0
	import sys
"8	LINE	0
	import glob
"9	LINE	0
	import commands
""-	CHAR	0
	"/sbin/ifconfig").split("\n")[1].split()[1][5:] #get the ip address

# File marks:
'0  19  65  ~/part2/README
'1  13  83  ~/part2/part2.sh
'2  6  13  ~/part2/CSV_sort.py
'3  5  102  ~/part2/CSV_sort.py
'4  6  0  ~/part2/part2.sh
'5  4  0  /etc/hosts
'6  2  33  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
'7  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
'8  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/workers
'9  1  0  /mnt/datahadoop-3.1.2/etc/hadoop/workers

# Jumplist (newest first):
-'  19  65  ~/part2/README
-'  1  0  ~/part2/README
-'  13  83  ~/part2/part2.sh
-'  1  0  ~/part2/part2.sh
-'  6  13  ~/part2/CSV_sort.py
-'  1  0  ~/part2/CSV_sort.py
-'  5  102  ~/part2/CSV_sort.py
-'  6  0  ~/part2/part2.sh
-'  4  0  /etc/hosts
-'  1  0  /etc/hosts
-'  2  33  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/workers
-'  1  0  /mnt/datahadoop-3.1.2/etc/hadoop/workers
-'  1  0  hadoop-3.1.2/etc/hadoop/workers
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/core-site.xml
-'  1  0  hadoop-3.1.2/etc/hadoop/core-site.xml
-'  9  445  ~/.ssh/authorized_keys
-'  1  0  ~/.ssh/authorized_keys
-'  3  8  ~/all_ips
-'  1  0  ~/all_ips
-'  7  0  ~/part2/CSV_sort.py
-'  12  76  ~/part2/part2.sh
-'  117  1  ~/.bashrc
-'  1  0  ~/.bashrc
-'  91  0  ~/.bashrc
-'  11  140  ~/part2/README
-'  56  0  ~/pagerank_custome_p.py
-'  1  0  ~/pagerank_custome_p.py
-'  20  67  ~/part2/README
-'  4  48  ~/part2/README
-'  14  38  ~/part2/CSV_sort.py
-'  17  104  ~/part2/part2.sh
-'  71  97  ~/pagerank_custome_p.py
-'  63  104  ~/pagerank_custome_p.py
-'  31  0  ~/pagerank_custome_p.py
-'  67  86  ~/pagerank_custome_p.py
-'  69  15  ~/pagerank_custome_p.py
-'  54  107  ~/pagerank_nopartition.py
-'  1  0  ~/pagerank_nopartition.py
-'  1  10  ~/.vimrc
-'  69  0  ~/pagerank_nopartition.py
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf
-'  24  0  /mnt/data/spark_logs/app-20190922184312-0000
-'  1  0  /mnt/data/spark_logs/app-20190922184312-0000
-'  1  0  /mnt/data/hadoop-3.1.2/sbin/start-dfs.sh
-'  2  8  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
-'  4  8  /mnt/data/hadoop-3.1.2/etc/hadoop/workers
-'  22  22  /mnt/data/hadoop-3.1.2/etc/hadoop/core-site.xml
-'  34  44  ~/mod3.py
-'  1  0  ~/mod3.py
-'  69  15  ~/mod3.py
-'  75  86  ~/mod3.py
-'  69  15  ~/pagerank_large_keyBy.py
-'  1  0  ~/pagerank_large_keyBy.py
-'  5  75  ~/run_one_job.sh
-'  1  0  ~/run_one_job.sh
-'  63  56  ~/pagerank_large_keyBy.py
-'  64  4  ~/pagerank_large_keyBy.py
-'  7  0  ~/pagerank_nopartition.py
-'  6  9  ~/pagerank_nopartition.py
-'  65  88  ~/pagerank_nopartition.py
-'  61  60  ~/pagerank_nopartition.py
-'  59  83  ~/pagerank_nopartition.py
-'  31  68  ~/pagerank_nopartition.py
-'  30  0  ~/pagerank_nopartition.py
-'  68  0  ~/pagerank_nopartition.py
-'  1  0  ~/nopartition.sh
-'  74  0  ~/pagerank_large_keyBy.py
-'  2  18  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/sbin/spark-defaults.conf
-'  3  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf
-'  30  21  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf.template
-'  1  0  /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf.template
-'  6  73  ~/part3_large.sh
-'  1  0  ~/part3_large.sh
-'  5  16  /mnt/data/hadoop-3.1.2/etc/hadoop/workers
-'  54  53  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  47  57  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  37  3  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  26  33  /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
-'  10  445  ~/.ssh/authorized_keys
-'  26  33  /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
-'  54  53  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  47  57  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  37  3  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
-'  26  33  /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
-'  1  0  /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
-'  3  8  ~/all_ips
-'  1  0  ~/all_ips
-'  117  1  ~/.bashrc
-'  1  0  ~/.bashrc
-'  13  83  ~/part2/part2.sh

# History of marks within files (newest to oldest):

> ~/part2/README
	"	19	65
	^	19	66
	.	18	132
	+	4	102
	+	15	0
	+	14	14
	+	10	0
	+	11	0
	+	15	150
	+	20	67
	+	12	46
	+	18	50
	+	16	139
	+	18	156
	+	19	65
	+	18	132

> ~/part2/part2.sh
	"	13	83
	^	12	77
	.	12	76
	+	1	11
	+	17	29
	+	13	0
	+	12	76

> ~/part2/CSV_sort.py
	"	6	13
	^	5	49
	.	5	48
	+	14	0
	+	9	28
	+	6	13
	+	5	48

> /etc/hosts
	"	4	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-env.sh
	"	2	33
	^	2	34
	.	3	0
	+	1	37
	+	3	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/slaves
	"	1	0
	^	2	9
	.	2	8
	+	1	34
	+	1	9
	+	2	8

> /mnt/data/hadoop-3.1.2/etc/hadoop/workers
	"	1	0
	^	4	9
	.	4	8
	+	5	16
	+	5	0
	+	4	8

> /mnt/datahadoop-3.1.2/etc/hadoop/workers
	"	1	0

> hadoop-3.1.2/etc/hadoop/workers
	"	1	0

> /mnt/data/hadoop-3.1.2/etc/hadoop/core-site.xml
	"	1	0
	^	22	23
	.	22	22
	+	20	10
	+	23	0
	+	22	22

> hadoop-3.1.2/etc/hadoop/core-site.xml
	"	1	0

> ~/.ssh/authorized_keys
	"	9	445
	^	9	446
	.	9	429
	+	10	429
	+	9	429

> ~/all_ips
	"	3	8
	^	3	9
	.	3	9
	+	1	9
	+	3	9

> ~/.bashrc
	"	117	1
	^	118	45
	.	118	44
	+	117	2
	+	119	0
	+	117	2
	+	118	44

> ~/pagerank_custome_p.py
	"	56	0
	^	71	98
	.	71	98
	+	63	102
	+	65	103
	+	71	100
	+	69	15
	+	74	21
	+	67	87
	+	63	101
	+	65	101
	+	71	98

> ~/pagerank_nopartition.py
	"	54	107
	^	1	4
	.	7	0
	+	1	75
	+	1	87
	+	1	17
	+	1	74
	+	1	0
	+	1	57
	+	1	65
	+	1	65
	+	1	65
	+	1	41
	+	1	69
	+	1	0
	+	7	0
	+	7	27
	+	67	0
	+	30	0
	+	31	69
	+	59	0
	+	59	83
	+	61	60
	+	65	88
	+	7	0
	+	6	0
	+	1	4
	+	7	0

> ~/.vimrc
	"	1	10
	^	1	11
	.	1	11
	+	1	11

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf
	"	1	0
	^	2	19
	.	2	19
	+	6	0
	+	3	0
	+	7	30
	+	2	19

> /mnt/data/spark_logs/app-20190922184312-0000
	"	24	0

> /mnt/data/hadoop-3.1.2/sbin/start-dfs.sh
	"	1	0

> ~/mod3.py
	"	34	44
	^	34	45
	.	34	44
	+	75	0
	+	74	0
	+	72	1
	+	69	15
	+	34	44

> ~/pagerank_large_keyBy.py
	"	69	15
	^	69	16
	.	69	15
	+	74	0
	+	69	15

> ~/run_one_job.sh
	"	5	75
	^	5	76
	.	5	75
	+	1	33
	+	10	0
	+	8	0
	+	7	0
	+	6	0
	+	1	7
	+	5	0
	+	1	9
	+	2	8
	+	5	67
	+	3	8
	+	5	75

> ~/nopartition.sh
	"	1	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/sbin/spark-defaults.conf
	"	1	0

> /mnt/data/spark-2.4.4-bin-hadoop2.7/conf/spark-defaults.conf.template
	"	30	21
	^	30	22
	.	29	21
	+	24	0
	+	23	0
	+	24	99
	+	26	36
	+	27	20
	+	23	23
	+	24	19
	+	26	20
	+	29	21

> ~/part3_large.sh
	"	6	73
	^	6	74
	.	6	73
	+	6	73

> /mnt/data/hadoop-3.1.2/etc/hadoop/hadoop-env.sh
	"	54	53
	^	54	54
	.	54	17
	+	54	17

> /mnt/data/hadoop-3.1.2/etc/hadoop/hdfs-site.xml
	"	26	33
	^	26	34
	.	26	33
	+	27	0
	+	22	44
	+	26	33
