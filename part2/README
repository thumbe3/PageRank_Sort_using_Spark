We assume that HDFS and Spark have already been configured before running this part.
We assume that the hdfs bin and sbin folders have been added to the PATH variable and the main spark directory is in the same location as the files of this directory.

Basically part2.sh, CSV_sort.py, README and spark-2.4.4-bin-hadoop2.7 are present in the same location

Once all this is ensured

run
./part2.sh $1 $2
	where $1 is the name of the input file(should be present in the same directory) (if not provided the script will download export.csv and consider it as the input file)
	where $2 is the name of the output folder where you want to save the output ( if not provided default name of the output folder is "output")

Please note again that both the above arguments are optional.

What the script run.sh does:
	The script takes the input file, puts it in hdfs. Then a spark-submit job is run that takes the hdfs file as input performs the composite-key sorting
	and stores the result in the hdfs folder. The script regains the control and gets the output from hdfs and stores it back in the local fs.

Important Note: ip_addr of spark master is found by executing ifconfig and running grep on the result for "eth1"(We assume that eth1 has the private ip address).
		 You can change that part of the code to use public ip as well, but the same change needs to be done in the CSV_sort.py file.
run.sh is appropraitely commented, you can find more details there
